{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2396b81",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Estimate $\\Pr(y=1 \\mid \\mathbf{x})$ for binary labels $y\\in\\{0,1\\}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"visualizations/logistic_regression.png\" width=\"600\">\n",
    "<img src=\"visualizations/logistic_regression2.png\" width=\"600\">\n",
    "\n",
    "\n",
    "* **Hypothesis**\n",
    "\n",
    "  $$\n",
    "    h_\\theta(\\mathbf{x}) = \\sigma(\\theta^\\top \\mathbf{x})\n",
    "    = \\frac{1}{1 + e^{-\\theta^\\top \\mathbf{x}}}.\n",
    "  $$\n",
    "\n",
    "* **Decision Rule**\n",
    "  Predict 1 if $h_\\theta(\\mathbf{x}) \\ge 0.5$; else 0.\n",
    "\n",
    "* **Loss Function**\n",
    "  Cross‐entropy (negative log-likelihood):\n",
    "\n",
    "  $$\n",
    "    J(\\theta)\n",
    "    = -\\frac{1}{m}\\sum_{i=1}^m \\bigl[y^{(i)}\\log h_\\theta(\\mathbf{x}^{(i)}) + (1-y^{(i)})\\log(1 - h_\\theta(\\mathbf{x}^{(i)}))\\bigr].\n",
    "  $$\n",
    "\n",
    "* **Optimization**\n",
    "  Gradient descent (or advanced optimizers):\n",
    "\n",
    "  $$\n",
    "    \\nabla J(\\theta)_j\n",
    "    = \\frac{1}{m}\\sum_i \\bigl(h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)}\\bigr)\\,x_j^{(i)}.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### ℓ₂ Regularization (Ridge Penalty)\n",
    "\n",
    "Adds $\\tfrac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2$ to $J(\\theta)$, which:\n",
    "\n",
    "* **Shrinks** weights toward zero (but not exactly to zero).\n",
    "* **Reduces variance**, helping prevent overfitting.\n",
    "* Keeps the overall cost convex and easy to optimize.\n",
    "\n",
    "---\n",
    "\n",
    "### Softmax (Multinomial Logistic)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"visualizations/logistic_regression_softmax.png\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "\n",
    "For $K$-class problems, generalize with parameters $\\{\\theta_k\\}$:\n",
    "\n",
    "$$\n",
    "  \\Pr(y=k\\mid \\mathbf{x})\n",
    "  = \\frac{\\exp(\\theta_k^\\top \\mathbf{x})}{\\sum_{j=1}^K \\exp(\\theta_j^\\top \\mathbf{x})}.\n",
    "$$\n",
    "\n",
    "Use the multiclass cross-entropy loss and jointly optimize all $\\theta_k$, yielding well-calibrated probabilities without overlapping decision “gaps.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf242c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from cifar10.unpickle import get_all_data, get_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449f637",
   "metadata": {},
   "source": [
    "### Load & Preprocess CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51cbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load\n",
    "x_train, y_train = get_all_data()\n",
    "x_test, y_test = get_test_data()\n",
    "\n",
    "# 2) Normalize to [0,1] and flatten\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "n_samples, h, w, c = x_train.shape  # h=32, w=32, c=3\n",
    "x_train = x_train.reshape(n_samples, h * w * c)  # flatten to (N, 3072)\n",
    "x_test = x_test.reshape(x_test.shape[0], h * w * c)  # flatten to (N, 3072)\n",
    "\n",
    "# 3) One-hot encode labels\n",
    "num_classes = 10\n",
    "def one_hot(y, K):\n",
    "    m = y.shape[0]\n",
    "    oh = np.zeros((m, K))\n",
    "    oh[np.arange(m), y] = 1\n",
    "    return oh\n",
    "# So if y = [3, 1, 0], then:\n",
    "# one_hot(y, 10) → [\n",
    "#   [0,0,0,1,0,0,0,0,0,0],\n",
    "#   [0,1,0,0,0,0,0,0,0,0],\n",
    "#   [1,0,0,0,0,0,0,0,0,0]\n",
    "# ]\n",
    "\n",
    "\n",
    "Y_train = one_hot(np.array(y_train), num_classes)\n",
    "Y_test  = one_hot(np.array(y_test),  num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefccf8f",
   "metadata": {},
   "source": [
    "## Model Components (Softmax & Loss -with ℓ₂)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db398715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    # logits: (batch, K)\n",
    "    exp = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def compute_loss_and_grad(X, Y, W, b, reg_strength):\n",
    "    \"\"\"\n",
    "    X: (batch, D), Y: (batch, K) one-hot\n",
    "    W: (D, K), b: (K,)\n",
    "    returns: loss (scalar), dW, db\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    logits = X.dot(W) + b          # (m, K)\n",
    "    P = softmax(logits)            # (m, K)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    data_loss = -np.sum(Y * np.log(P + 1e-12)) / m\n",
    "    \n",
    "    # ℓ₂ penalty (exclude bias)\n",
    "    reg_loss = 0.5 * reg_strength * np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    \n",
    "    # Gradient\n",
    "    dlogits = (P - Y) / m          # (m, K)\n",
    "    dW = X.T.dot(dlogits)          # (D, K)\n",
    "    db = dlogits.sum(axis=0)       # (K,)\n",
    "    \n",
    "    # add regularization gradient\n",
    "    dW += reg_strength * W\n",
    "    return loss, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fbcf22",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Mini-batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fbd11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — loss: 1.9981, train_acc: 0.297, val_acc: 0.299\n",
      "Epoch 5/30 — loss: 1.8212, train_acc: 0.368, val_acc: 0.362\n",
      "Epoch 10/30 — loss: 1.8155, train_acc: 0.377, val_acc: 0.365\n",
      "Epoch 15/30 — loss: 1.8038, train_acc: 0.385, val_acc: 0.381\n",
      "Epoch 20/30 — loss: 1.7696, train_acc: 0.392, val_acc: 0.385\n",
      "Epoch 25/30 — loss: 1.7475, train_acc: 0.397, val_acc: 0.392\n",
      "Epoch 30/30 — loss: 1.8043, train_acc: 0.401, val_acc: 0.395\n"
     ]
    }
   ],
   "source": [
    "def train(X, Y, X_val, Y_val,\n",
    "          lr=1e-2, reg=1e-3,\n",
    "          batch_size=256, epochs=20):\n",
    "    D, K = X.shape[1], Y.shape[1]\n",
    "    # Initialize parameters\n",
    "    W = 0.001 * np.random.randn(D, K)\n",
    "    b = np.zeros(K)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        X_shuf, Y_shuf = X[perm], Y[perm]\n",
    "        \n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            xb = X_shuf[i:i+batch_size]\n",
    "            yb = Y_shuf[i:i+batch_size]\n",
    "            loss, dW, db = compute_loss_and_grad(xb, yb, W, b, reg)\n",
    "            \n",
    "            # Parameter update\n",
    "            W -= lr * dW\n",
    "            b -= lr * db\n",
    "        \n",
    "        # Evaluate train/val accuracy\n",
    "        if (epoch+1) % 5 == 0 or epoch==0:\n",
    "            def predict_acc(X, Y):\n",
    "                probs = softmax(X.dot(W) + b)\n",
    "                preds = np.argmax(probs, axis=1)\n",
    "                return np.mean(preds == np.argmax(Y, axis=1))\n",
    "            train_acc = predict_acc(X, Y)\n",
    "            val_acc   = predict_acc(X_val, Y_val)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} — loss: {loss:.4f}, \"\n",
    "                  f\"train_acc: {train_acc:.3f}, val_acc: {val_acc:.3f}\")\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "# Example usage:\n",
    "W, b = train(x_train, Y_train, x_test, Y_test,\n",
    "             lr=1e-2, reg=1e-3,\n",
    "             batch_size=512, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980e9e5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px;\">Integrate filters</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a4ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from images.image_preprocessing import (\n",
    "    extract_raw_pixels,\n",
    "    extract_color_histogram,\n",
    "    extract_hog,\n",
    "    extract_lbp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0186054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract(fn, X):\n",
    "    \"\"\"Apply single‐image fn over a batch X of shape (n,H,W,C).\"\"\"\n",
    "    return np.stack([fn(im) for im in X], axis=0)\n",
    "\n",
    "\n",
    "x_train, y_train = get_all_data()\n",
    "x_test, y_test = get_test_data()\n",
    "\n",
    "Xraw_train = batch_extract(extract_raw_pixels, x_train)\n",
    "Xraw_test = batch_extract(extract_raw_pixels, x_test)\n",
    "\n",
    "Xhist_train = batch_extract(extract_color_histogram, x_train)\n",
    "Xhist_test = batch_extract(extract_color_histogram, x_test)\n",
    "\n",
    "Xhog_train = batch_extract(extract_hog, x_train)\n",
    "Xhog_test = batch_extract(extract_hog, x_test)\n",
    "\n",
    "Xlbp_train = batch_extract(extract_lbp, x_train)\n",
    "Xlbp_test = batch_extract(extract_lbp, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46f6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([Xraw_train, Xhist_train, Xhog_train, Xlbp_train], axis=1)\n",
    "X_test  = np.concatenate([Xraw_test,  Xhist_test,  Xhog_test,  Xlbp_test], axis=1)\n",
    "\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0) + 1e-10  # avoid division by zero\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test  = (X_test  - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e7e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — loss: 1.3658, train_acc: 0.570, val_acc: 0.546\n",
      "Epoch 5/30 — loss: 1.0555, train_acc: 0.651, val_acc: 0.608\n",
      "Epoch 10/30 — loss: 1.0408, train_acc: 0.682, val_acc: 0.621\n",
      "Epoch 15/30 — loss: 0.8835, train_acc: 0.694, val_acc: 0.622\n",
      "Epoch 20/30 — loss: 0.9000, train_acc: 0.704, val_acc: 0.621\n",
      "Epoch 25/30 — loss: 0.8704, train_acc: 0.710, val_acc: 0.624\n",
      "Epoch 30/30 — loss: 0.9066, train_acc: 0.716, val_acc: 0.623\n"
     ]
    }
   ],
   "source": [
    "# Your feature extraction + preprocessing steps here (batch_extract calls, concatenation, scaling)...\n",
    "\n",
    "# Then prepare labels as before:\n",
    "y_train = np.array(y_train).flatten()\n",
    "y_test  = np.array(y_test).flatten()\n",
    "Y_train = one_hot(y_train, num_classes)\n",
    "Y_test  = one_hot(y_test, num_classes)\n",
    "\n",
    "# Train with the processed features\n",
    "W, b = train(X_train, Y_train, X_test, Y_test,\n",
    "             lr=1e-2, reg=1e-3,\n",
    "             batch_size=512, epochs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe67d6b",
   "metadata": {},
   "source": [
    "## Just HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — loss: 1.7308, train_acc: 0.456, val_acc: 0.450\n",
      "Epoch 5/30 — loss: 1.4953, train_acc: 0.504, val_acc: 0.496\n",
      "Epoch 10/30 — loss: 1.4324, train_acc: 0.516, val_acc: 0.506\n",
      "Epoch 15/30 — loss: 1.3030, train_acc: 0.521, val_acc: 0.514\n",
      "Epoch 20/30 — loss: 1.5365, train_acc: 0.524, val_acc: 0.514\n",
      "Epoch 25/30 — loss: 1.3289, train_acc: 0.526, val_acc: 0.516\n",
      "Epoch 30/30 — loss: 1.3856, train_acc: 0.528, val_acc: 0.516\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "x_train, y_train = get_all_data()\n",
    "x_test, y_test = get_test_data()\n",
    "\n",
    "# Extract HOG features only\n",
    "X_train = batch_extract(extract_hog, x_train)\n",
    "X_test  = batch_extract(extract_hog, x_test)\n",
    "\n",
    "# Normalize features\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0) + 1e-10  # avoid division by zero\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test  = (X_test  - mean) / std\n",
    "\n",
    "# Prepare labels\n",
    "y_train = np.array(y_train).flatten()\n",
    "y_test  = np.array(y_test).flatten()\n",
    "\n",
    "num_classes = 10  # CIFAR-10 classes\n",
    "\n",
    "Y_train = one_hot(y_train, num_classes)\n",
    "Y_test  = one_hot(y_test, num_classes)\n",
    "\n",
    "# Define train and other functions (assumed imported or defined elsewhere)\n",
    "W, b = train(X_train, Y_train, X_test, Y_test, lr=1e-2, reg=1e-3, batch_size=512, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66de9a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
