{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a656cb",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "1. **Bayesâ€™ Theorem**\n",
    "\n",
    "   $$\n",
    "   P(C \\mid \\mathbf{x})\n",
    "   = \\frac{P(\\mathbf{x}\\mid C)\\,P(C)}{P(\\mathbf{x})}\n",
    "   $$\n",
    "\n",
    "   * We compare $P(\\mathbf{x}\\mid C)\\,P(C)$ across classes (the denominator is common).\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **â€œNaiveâ€ Independence**\n",
    "\n",
    "   $$\n",
    "   P(\\mathbf{x}\\mid C)\n",
    "   = \\prod_{i=1}^n P(x_i\\mid C)\n",
    "   $$\n",
    "\n",
    "   * Cuts parameter count from $O(k^n)$ to $O(n\\,k)$.\n",
    "   * Makes training feasible for high-dimensional data.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Training**\n",
    "\n",
    "   * **Priors**: $P(C)=\\tfrac{\\text{count}(C)}{m}$.\n",
    "   * **Likelihoods**:\n",
    "\n",
    "     * *Categorical*: $\\displaystyle P(x_i=v\\mid C)\\!=\\!\\frac{N_{i,C}(v)+\\alpha}{N_C+\\alpha\\,V_i}.$\n",
    "     * *Gaussian*: estimate $\\mu_{i,C}, \\sigma^2_{i,C}$ per feature.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Prediction**\n",
    "\n",
    "   $$\n",
    "   \\log P(C\\mid \\mathbf{x})\n",
    "   \\;\\propto\\;\n",
    "   \\log P(C)\n",
    "   +\\sum_{i=1}^n \\log P(x_i\\mid C)\n",
    "   $$\n",
    "\n",
    "   * Choose the class with the highest log-score.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Common Variants**\n",
    "\n",
    "|                                 Variant                                 |                                                                            When to use                                                                            |                                                                 Likelihood Model                                                                |\n",
    "| :---------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------: |\n",
    "|                             **Multinomial**                             |       When features are **counts or frequencies** (e.g., number of times a word appears). Best for **text classification** with Bag-of-Words or TF vectors.       |  $P(x_i = k \\mid C) = \\displaystyle\\frac{\\text{count of }i\\text{ in }C + \\alpha}{\\sum_j \\bigl(\\text{count of }j\\text{ in }C\\bigr) + \\alpha V}$  |\n",
    "|                              **Bernoulli**                              |                       When features are **binary** (e.g., whether a word is present or not). Good for **short text** or sparse binary input.                      |                                    $P(x_i = 1 \\mid C) = \\displaystyle\\frac{N_{i,C} + \\alpha}{N_C + 2\\alpha}$                                    |\n",
    "|                               **Gaussian**                              |           When features are **continuous and approximately normally distributed**. Common in **sensor data**, **medical data**, and numerical features.           |        $P(x_i \\mid C) = \\displaystyle\\frac{1}{\\sqrt{2\\pi\\sigma_{i,C}^2}}\\exp\\!\\Bigl(-\\tfrac{(x_i - \\mu_{i,C})^2}{2\\sigma_{i,C}^2}\\Bigr)$        |\n",
    "|                                 **Beta**                                |  When features are **real values constrained in $[0,1]$** (e.g., proportions, probabilities, normalized pixel intensities). Useful when Gaussian is too flexible. |                 $P(x_i \\mid C) = \\displaystyle\\frac{x_i^{\\alpha_{i,C}-1}(1-x_i)^{\\beta_{i,C}-1}}{B(\\alpha_{i,C},\\,\\beta_{i,C})}$                |\n",
    "|                         **Mixture of Gaussians**                        | When feature distributions are **multi-modal** and you need more flexibility than a single Gaussianâ€”still assumes **independence** if using diagonal covariances. |              $P(x_i \\mid C) = \\displaystyle\\sum_{m=1}^{M} w_{i,C,m}\\;\\mathcal{N}\\bigl(x_i\\mid\\mu_{i,C,m},\\,\\sigma^2_{i,C,m}\\bigr)$,             |\n",
    "|                                 **KDE**                                 |   When you want **no strong assumption** on the data distribution. Itâ€™s a **non-parametric** method that places a kernel at **every training point**, so it can capture arbitrarily complex shapes but is **computationally expensive** at inference.   | $P(x_i \\mid C) = \\displaystyle\\frac{1}{n\\,h\\,\\sqrt{2\\pi}}\\sum_{j=1}^{n}\\exp\\!\\Bigl(-\\tfrac{(x_i - x_{ij})^2}{2h^2}\\Bigr)$ for a Gaussian kernel |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "6. **Pros & Cons**\n",
    "\n",
    "   * **Pros**: Fast to train/predict, works with high-dimensional sparse data, needs little data.\n",
    "   * **Cons**: Independence assumption often violated; zero-frequency issues (solved by smoothing).\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "> **Extra note (why logs?):**\n",
    "> Multiplying many small $P(x_i\\mid C)$ can underflow to zero. By taking $\\log$, you convert products into sums, avoid underflow/overflow, speed up computation, and keep the same class ordering since $\\log$ is monotonic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7d0ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from unpickle_CIFAR10 import get_data, get_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95628ead",
   "metadata": {},
   "source": [
    "### Load & Preprocess CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7bd7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load\n",
    "x_train, y_train = get_data(1)\n",
    "x_test, y_test = get_test_data()\n",
    "\n",
    "# 2) Normalize to [0,1] and flatten\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "n_samples, h, w, c = x_train.shape  # h=32, w=32, c=3\n",
    "x_train = x_train.reshape(n_samples, h * w * c)  # flatten to (N, 3072)\n",
    "x_test = x_test.reshape(x_test.shape[0], h * w * c)  # flatten to (N, 3072)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ba38e",
   "metadata": {},
   "source": [
    "### Estimate Class Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c993e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "n_classes = len(classes)\n",
    "\n",
    "# Count how many training samples per class and divide by total\n",
    "priors = np.array([np.mean(y_train == c) for c in np.unique(y_train)])  # Prior probabilities P(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017293d",
   "metadata": {},
   "source": [
    "### Compute Feature-Conditional Statistics\n",
    "Gaussian assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ad14bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each class ğ¶ and pixel-feature ğ‘–, compute the sample mean and variance\n",
    "means = np.zeros((n_classes, x_train.shape[1]), dtype=np.float32)\n",
    "vars_ = np.zeros((n_classes, x_train.shape[1]), dtype=np.float32)\n",
    "\n",
    "for idx, c in enumerate(classes):\n",
    "    # select samples of class c\n",
    "    x_c = x_train[y_train == c]\n",
    "\n",
    "    means[idx, :] = x_c.mean(axis=0)  # Î¼_{i,C}\n",
    "    vars_[idx, :] = x_c.var(axis=0)  # ÏƒÂ²_{i,C}\n",
    "\n",
    "\n",
    "def log_gaussian_pdf(x, mean, var):\n",
    "    return -0.5 * (np.log(2 * np.pi * var) + ((x - mean) ** 2 / var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc1281d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 test accuracy (Gaussian NB from scratch): 29.30%\n"
     ]
    }
   ],
   "source": [
    "def predict(x):\n",
    "    log_probs = np.log(priors)  # log P(C) \n",
    "    for idx in range(n_classes):\n",
    "        log_probs[idx] += np.sum(log_gaussian_pdf(x, means[idx], vars_[idx]))\n",
    "    return classes[np.argmax(log_probs)]\n",
    "\n",
    "\n",
    "# Vectorized prediction on test set\n",
    "y_pred = np.array([predict(x) for x in x_test])\n",
    "\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"CIFAR-10 test accuracy (Gaussian NB from scratch): {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeecef",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px;\">Compare variants</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e281e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1) Gaussian Naive Bayes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def train_gaussian_nb(x, y):\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    n_features = x.shape[1]\n",
    "\n",
    "    means  = np.zeros((n_classes, n_features), dtype=np.float32)\n",
    "    vars_  = np.zeros((n_classes, n_features), dtype=np.float32)\n",
    "    for idx, c in enumerate(classes):\n",
    "        xc = x[y == c]\n",
    "        means[idx] = xc.mean(axis=0)\n",
    "        vars_[idx] = xc.var(axis=0) + 1e-9\n",
    "    return means, vars_\n",
    "\n",
    "def predict_gaussian(x, priors, means, vars_):\n",
    "    logp = np.log(priors).copy()\n",
    "    for idx in range(n_classes):\n",
    "        m, v = means[idx], vars_[idx]\n",
    "        logp[idx] += np.sum(-0.5*(np.log(2*np.pi*v) + ((x-m)**2)/v))\n",
    "    return classes[np.argmax(logp)]\n",
    "\n",
    "\n",
    "# â”€â”€ 2) Bernoulli Naive Bayes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def train_bernoulli_nb(x, y, thr=0.5, alpha=1.0):\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    n_features = x.shape[1]\n",
    "\n",
    "    xb = (x > thr).astype(int)\n",
    "    probs  = np.zeros((n_classes, n_features))\n",
    "    for idx, c in enumerate(classes):\n",
    "        xc = xb[y == c]\n",
    "        probs[idx] = (xc.sum(axis=0) + alpha) / (len(xc) + 2*alpha)\n",
    "    return probs\n",
    "\n",
    "def predict_bernoulli(x, priors, probs, thr=0.5):\n",
    "    xb = (x > thr).astype(int)\n",
    "    logp = np.log(priors).copy()\n",
    "    for idx in range(n_classes):\n",
    "        p = probs[idx]\n",
    "        logp[idx] += np.sum(xb * np.log(p) + (1-xb)*np.log(1-p))\n",
    "    return classes[np.argmax(logp)]\n",
    "\n",
    "\n",
    "# â”€â”€ 3) Multinomial Naive Bayes (with discretization) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def train_multinomial_nb(x, y, bins=8, alpha=1.0):\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    n_features = x.shape[1]\n",
    "\n",
    "    edges = np.linspace(0, 1, bins+1)\n",
    "    xd = np.digitize(x, bins=edges) - 1\n",
    "    cond = np.zeros((n_classes, n_features, bins))\n",
    "    for idx, c in enumerate(classes):\n",
    "        xc = xd[y == c]\n",
    "        counts = np.stack([np.sum(xc==k, axis=0) for k in range(bins)], axis=1)\n",
    "        cond[idx] = (counts + alpha) / (len(xc) + alpha*bins)\n",
    "    return cond, edges\n",
    "\n",
    "def predict_multinomial(x, priors, cond, edges):\n",
    "    bins = cond.shape[2]\n",
    "    xd = np.clip(np.digitize(x, bins=edges)-1, 0, bins-1)\n",
    "    n_features = x.shape[0]\n",
    "    logp = np.log(priors).copy()\n",
    "    for idx in range(n_classes):\n",
    "        logp[idx] += np.sum(np.log(cond[idx, np.arange(n_features), xd]))\n",
    "    return classes[np.argmax(logp)]\n",
    "\n",
    "\n",
    "# â”€â”€ 4) Beta-distribution Naive Bayes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from scipy.special import betaln\n",
    "\n",
    "def train_beta_nb(x, y, eps=1e-3):\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    n_features = x.shape[1]\n",
    "\n",
    "    alphas = np.zeros((n_classes, n_features))\n",
    "    betas  = np.zeros((n_classes, n_features))\n",
    "    for idx, c in enumerate(classes):\n",
    "        xc = np.clip(x[y==c], eps, 1-eps)\n",
    "        mu = xc.mean(axis=0)\n",
    "        var = xc.var(axis=0) + 1e-9\n",
    "        tmp = (mu*(1-mu)/var) - 1\n",
    "        alphas[idx] = np.maximum(mu*tmp, eps)\n",
    "        betas[idx]  = np.maximum((1-mu)*tmp, eps)\n",
    "    return alphas, betas\n",
    "\n",
    "def predict_beta(x, priors, alphas, betas, eps=1e-6):\n",
    "    x = np.clip(x, eps, 1-eps)\n",
    "    logp = np.log(priors).copy()\n",
    "    for idx in range(n_classes):\n",
    "        a, b = alphas[idx], betas[idx]\n",
    "        log_pdf = (a-1)*np.log(x) + (b-1)*np.log(1-x) - betaln(a, b)\n",
    "        logp[idx] += np.sum(log_pdf)\n",
    "    return classes[np.argmax(logp)]\n",
    "\n",
    "\n",
    "# â”€â”€ 5) Mixture-of-Gaussians Naive Bayes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def train_mog_nb(x, y, n_components=3):\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    n_features = x.shape[1]\n",
    "\n",
    "    gmms   = []\n",
    "    for c in classes:\n",
    "        xc = x[y == c]  # shape (N_c, n_features)\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=n_components,\n",
    "            covariance_type='diag',\n",
    "            max_iter=100,\n",
    "            random_state=0\n",
    "        )\n",
    "        gmm.fit(xc)\n",
    "        gmms.append(gmm)\n",
    "    return gmms\n",
    "\n",
    "def predict_mog_nb(x, priors, gmms):\n",
    "    logp = np.log(priors).copy()\n",
    "    for idx, gmm in enumerate(gmms):\n",
    "        # gmm.score_samples returns logâ€‰p(x|class)\n",
    "        logp[idx] += gmm.score_samples(x.reshape(1, -1))[0]\n",
    "    return classes[np.argmax(logp)]\n",
    "\n",
    "\n",
    "# # â”€â”€ 6) KDE-NB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# def train_kde_nb(x, y, kernel_bandwidth=None):\n",
    "    # classes = np.unique(y)\n",
    "    # n_classes = len(classes)\n",
    "    # n_features = x.shape[1]\n",
    "\n",
    "#     # collect per-class training values\n",
    "#     values = [x[y==c] for c in classes]  # each is (N_c, n_features)\n",
    "#     # estimate bandwidth per class+feature if not given\n",
    "#     if kernel_bandwidth is None:\n",
    "#         kernel_bandwidth = []\n",
    "#         for xc in values:\n",
    "#             # Silvermanâ€™s rule per feature\n",
    "#             std = xc.std(axis=0) + 1e-9\n",
    "#             n_c = xc.shape[0]\n",
    "#             h   = 1.06 * std * (n_c ** -0.2)\n",
    "#             kernel_bandwidth.append(h)\n",
    "#     return values, kernel_bandwidth\n",
    "\n",
    "# def predict_kde(x, priors, values, bandwidths):\n",
    "#     logp = np.log(priors).copy()\n",
    "#     for idx in range(n_classes):\n",
    "#         xc = values[idx]      # (N_c, n_features)\n",
    "#         h  = bandwidths[idx]  # (n_features,)\n",
    "#         # compute per-feature KDE estimate\n",
    "#         # P(x_i|C) = 1/(N_c * h_i * sqrt(2Ï€)) * sum_j exp(-0.5*((x_i-xc_j)/h_i)^2)\n",
    "#         diffs = (x - xc[:,None]) / h[None,:,]      # shape (N_c, n_features)\n",
    "#         kernel_vals = np.exp(-0.5 * diffs**2)      # same shape\n",
    "#         # sum over training points\n",
    "#         sums = kernel_vals.sum(axis=0)             # (n_features,)\n",
    "#         logp[idx] += np.sum(np.log(sums) - np.log(xc.shape[0]*h*np.sqrt(2*np.pi)))\n",
    "#     return classes[np.argmax(logp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b372a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gaussian: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:01<00:00, 6637.43it/s]\n",
      "Bernoulli: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:02<00:00, 4197.23it/s]\n",
      "Multinomial: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:02<00:00, 4042.31it/s]\n",
      "Beta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:11<00:00, 846.13it/s]\n",
      "Mix-Gaussians: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:08<00:00, 1140.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Test accuracies on CIFAR-10:\n",
      " â€¢ Gaussian NB:      29.30%\n",
      " â€¢ Bernoulli NB:     27.71%\n",
      " â€¢ Multinomial NB:   30.15%\n",
      " â€¢ Beta NB:          28.45%\n",
      " â€¢ MoG NB:          32.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ RUN & COMPARE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# priors\n",
    "priors = np.array([np.mean(y_train == c) for c in classes])\n",
    "\n",
    "means_g, vars_g = train_gaussian_nb(x_train, y_train)\n",
    "probs_b = train_bernoulli_nb(x_train, y_train)\n",
    "cond_m, edges_m = train_multinomial_nb(x_train, y_train, bins=8)\n",
    "alphas_bt, betas_bt = train_beta_nb(x_train, y_train)\n",
    "gmms = train_mog_nb(x_train, y_train, n_components=3)\n",
    "# values_k, bw_k = train_kde_nb(x_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_g   = np.array([predict_gaussian(x, priors, means_g, vars_g)         for x in tqdm(x_test, desc=\"Gaussian\")])\n",
    "y_pred_b   = np.array([predict_bernoulli(x, priors, probs_b)            for x in tqdm(x_test, desc=\"Bernoulli\")])\n",
    "y_pred_m   = np.array([predict_multinomial(x, priors, cond_m, edges_m)          for x in tqdm(x_test, desc=\"Multinomial\")])\n",
    "y_pred_bt  = np.array([predict_beta(x, priors, alphas_bt, betas_bt)         for x in tqdm(x_test, desc=\"Beta\")])\n",
    "y_pred_mog = np.array([predict_mog_nb(x, priors, gmms)          for x in tqdm(x_test, desc=\"Mix-Gaussians\")])\n",
    "# y_pred_kde = np.array([predict_kde(x, priors, values_k, bw_k)           for x in tqdm(x_test, desc=\"KDE\")])\n",
    "\n",
    "# accuracies\n",
    "acc_g   = np.mean(y_pred_g   == y_test)\n",
    "acc_b   = np.mean(y_pred_b   == y_test)\n",
    "acc_m   = np.mean(y_pred_m   == y_test)\n",
    "acc_bt  = np.mean(y_pred_bt  == y_test)\n",
    "acc_mog = np.mean(y_pred_mog == y_test)\n",
    "# acc_kde = np.mean(y_pred_kde == y_test)\n",
    "\n",
    "print(\" \\n Test accuracies on CIFAR-10:\")\n",
    "print(f\" â€¢ Gaussian NB:     {acc_g*100:6.2f}%\")\n",
    "print(f\" â€¢ Bernoulli NB:    {acc_b*100:6.2f}%\")\n",
    "print(f\" â€¢ Multinomial NB:  {acc_m*100:6.2f}%\")\n",
    "print(f\" â€¢ Beta NB:         {acc_bt*100:6.2f}%\")\n",
    "print(f\" â€¢ MoG NB:          {acc_mog*100:.2f}%\")\n",
    "# print(f\" â€¢ KDE NB:          {acc_kde*100:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0eddf9",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px;\">Integrate filters</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543cbfb",
   "metadata": {},
   "source": [
    "# All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13a804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_preprocessing import (\n",
    "    extract_raw_pixels,\n",
    "    extract_color_histogram,\n",
    "    extract_hog,\n",
    "    extract_lbp,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d2f5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract(fn, X):\n",
    "    \"\"\"Apply singleâ€image fn over a batch X of shape (n,H,W,C).\"\"\"\n",
    "    return np.stack([fn(im) for im in X], axis=0)\n",
    "\n",
    "\n",
    "x_train, y_train = get_data(1)\n",
    "x_test, y_test = get_test_data()\n",
    "\n",
    "Xraw_train = batch_extract(extract_raw_pixels, x_train)\n",
    "Xraw_test = batch_extract(extract_raw_pixels, x_test)\n",
    "\n",
    "Xhist_train = batch_extract(extract_color_histogram, x_train)\n",
    "Xhist_test = batch_extract(extract_color_histogram, x_test)\n",
    "\n",
    "Xhog_train = batch_extract(extract_hog, x_train)\n",
    "Xhog_test = batch_extract(extract_hog, x_test)\n",
    "\n",
    "Xlbp_train = batch_extract(extract_lbp, x_train)\n",
    "Xlbp_test = batch_extract(extract_lbp, x_test)\n",
    "\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "# Raw & HOG & LBP: zeroâ€mean, unitâ€var\n",
    "def fit_standard(X):\n",
    "    mu   = X.mean(axis=0)\n",
    "    sigma= X.std(axis=0) + eps\n",
    "    return mu, sigma\n",
    "\n",
    "def transform_standard(X, mu, sigma):\n",
    "    return (X - mu) / sigma\n",
    "\n",
    "mu_raw, sd_raw = fit_standard(Xraw_train)\n",
    "Xraw_train_standard = transform_standard(Xraw_train, mu_raw, sd_raw)\n",
    "Xraw_test_standard = transform_standard(Xraw_test, mu_raw, sd_raw)\n",
    "\n",
    "mu_hog, sd_hog = fit_standard(Xhog_train)\n",
    "Xhog_train_standard = transform_standard(Xhog_train, mu_hog, sd_hog)\n",
    "Xhog_test_standard = transform_standard(Xhog_test, mu_hog, sd_hog)\n",
    "\n",
    "mu_lbp, sd_lbp = fit_standard(Xlbp_train)\n",
    "Xlbp_train_standard = transform_standard(Xlbp_train, mu_lbp, sd_lbp)\n",
    "Xlbp_test_standard = transform_standard(Xlbp_test, mu_lbp, sd_lbp)\n",
    "\n",
    "# Color histogram: minâ€“max to [0,1]\n",
    "def fit_minmax(X):\n",
    "    lo = X.min(axis=0)\n",
    "    hi = X.max(axis=0)\n",
    "    return lo, hi\n",
    "\n",
    "def transform_minmax(X, lo, hi):\n",
    "    return (X - lo) / (hi - lo + eps)\n",
    "\n",
    "lo_hist, hi_hist = fit_minmax(Xhist_train)\n",
    "Xhist_train_minmax = transform_minmax(Xhist_train, lo_hist, hi_hist)\n",
    "Xhist_test_minmax = transform_minmax(Xhist_test, lo_hist, hi_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0ab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gaussian: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:04<00:00, 2382.79it/s]\n",
      "Bernoulli: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:05<00:00, 1952.63it/s]\n",
      "Multinomial: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:06<00:00, 1639.62it/s]\n",
      "Beta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:28<00:00, 352.81it/s]\n",
      "Mix-Gaussians: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:14<00:00, 699.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Test accuracies on CIFAR-10:\n",
      " â€¢ Gaussian NB:      23.17%\n",
      " â€¢ Bernoulli NB:     27.94%\n",
      " â€¢ Multinomial NB:   18.53%\n",
      " â€¢ Beta NB:          10.12%\n",
      " â€¢ MoG NB:          28.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_all = np.hstack([Xraw_train_standard, Xhog_train_standard, Xlbp_train_standard, Xhist_train_minmax])\n",
    "X_test_all  = np.hstack([Xraw_test_standard, Xhog_test_standard, Xlbp_test_standard, Xhist_test_minmax])\n",
    "\n",
    "# â”€â”€ RUN & COMPARE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# priors\n",
    "classes = np.unique(y_train)\n",
    "priors = np.array([np.mean(y_train == c) for c in classes])\n",
    "\n",
    "\n",
    "means_g, vars_g = train_gaussian_nb(X_train_all, y_train)\n",
    "probs_b = train_bernoulli_nb(X_train_all, y_train)\n",
    "cond_m, edges_m = train_multinomial_nb(X_train_all, y_train, bins=8)\n",
    "alphas_bt, betas_bt = train_beta_nb(X_train_all, y_train)\n",
    "gmms = train_mog_nb(X_train_all, y_train, n_components=3)\n",
    "# values_k, bw_k = train_kde_nb(X_train_all, y_train)\n",
    "\n",
    "# predictions\n",
    "classes = np.unique(y_train)\n",
    "n_classes = len(classes)\n",
    "\n",
    "y_pred_g   = np.array([predict_gaussian(x, priors, means_g, vars_g)         for x in tqdm(X_test_all, desc=\"Gaussian\")])\n",
    "y_pred_b   = np.array([predict_bernoulli(x, priors, probs_b)            for x in tqdm(X_test_all, desc=\"Bernoulli\")])\n",
    "y_pred_m   = np.array([predict_multinomial(x, priors, cond_m, edges_m)          for x in tqdm(X_test_all, desc=\"Multinomial\")])\n",
    "y_pred_bt  = np.array([predict_beta(x, priors, alphas_bt, betas_bt)         for x in tqdm(X_test_all, desc=\"Beta\")])\n",
    "y_pred_mog = np.array([predict_mog_nb(x, priors, gmms)          for x in tqdm(X_test_all, desc=\"Mix-Gaussians\")])\n",
    "# y_pred_kde = np.array([predict_kde(x, priors, values_k, bw_k)           for x in tqdm(X_test_all, desc=\"KDE\")])\n",
    "\n",
    "# accuracies\n",
    "acc_g   = np.mean(y_pred_g   == y_test)\n",
    "acc_b   = np.mean(y_pred_b   == y_test)\n",
    "acc_m   = np.mean(y_pred_m   == y_test)\n",
    "acc_bt  = np.mean(y_pred_bt  == y_test)\n",
    "acc_mog = np.mean(y_pred_mog == y_test)\n",
    "# acc_kde = np.mean(y_pred_kde == y_test)\n",
    "\n",
    "print(\" \\n Test accuracies on CIFAR-10:\")\n",
    "print(f\" â€¢ Gaussian NB:     {acc_g*100:6.2f}%\")\n",
    "print(f\" â€¢ Bernoulli NB:    {acc_b*100:6.2f}%\")\n",
    "print(f\" â€¢ Multinomial NB:  {acc_m*100:6.2f}%\")\n",
    "print(f\" â€¢ Beta NB:         {acc_bt*100:6.2f}%\")\n",
    "print(f\" â€¢ MoG NB:          {acc_mog*100:.2f}%\")\n",
    "# print(f\" â€¢ KDE NB:          {acc_kde*100:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1668d3",
   "metadata": {},
   "source": [
    "# Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea66ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw-Gaussian: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:01<00:00, 6814.78it/s]\n",
      "Hist-Bernoulli: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:03<00:00, 3128.58it/s]\n",
      "HOG-Multinomial: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 17391.28it/s]\n",
      "LBP-Beta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 15853.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracies on CIFAR-10:\n",
      " â€¢ Raw (Gaussian NB):        29.30%\n",
      " â€¢ Color Hist (Bernoulli):   17.11%\n",
      " â€¢ HOG (Multinomial):        44.08%\n",
      " â€¢ LBP (Beta NB):            26.27%\n",
      " â€¢ Ensemble (majority vote): 34.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def majority_vote(preds):\n",
    "    stacked = np.vstack(preds).T \n",
    "    majority = []\n",
    "    for row in stacked:\n",
    "        counts = {}\n",
    "        for label in row:\n",
    "            counts[label] = counts.get(label, 0) + 1\n",
    "        majority_label = max(counts, key=counts.get)\n",
    "        majority.append(majority_label)\n",
    "    return np.array(majority)\n",
    "\n",
    "\n",
    "\n",
    "means_raw, vars_raw = train_gaussian_nb(Xraw_train.reshape(len(y_train), -1), y_train)\n",
    "probs_hist = train_bernoulli_nb(Xhist_train, y_train)\n",
    "cond_hog, edges_hog = train_multinomial_nb(Xhog_train, y_train)\n",
    "alphas_lbp, betas_lbp = train_beta_nb(Xlbp_train, y_train)\n",
    "\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "n_classes = len(classes)\n",
    "\n",
    "y_pred_raw = np.array([\n",
    "    predict_gaussian(x.reshape(-1), priors, means_raw, vars_raw)\n",
    "    for x in tqdm(Xraw_test.reshape(len(y_test), -1), desc=\"Raw-Gaussian\")\n",
    "])\n",
    "y_pred_hist = np.array([\n",
    "    predict_bernoulli(x, priors, probs_hist)\n",
    "    for x in tqdm(Xhist_test, desc=\"Hist-Bernoulli\")\n",
    "])\n",
    "y_pred_hog = np.array([\n",
    "    predict_multinomial(x, priors, cond_hog, edges_hog)\n",
    "    for x in tqdm(Xhog_test, desc=\"HOG-Multinomial\")\n",
    "])\n",
    "y_pred_lbp = np.array([\n",
    "    predict_beta(x, priors, alphas_lbp, betas_lbp)\n",
    "    for x in tqdm(Xlbp_test, desc=\"LBP-Beta\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "y_pred_ensemble = majority_vote([y_pred_raw, y_pred_hist, y_pred_hog, y_pred_lbp])\n",
    "\n",
    "acc_raw      = np.mean(y_pred_raw == y_test)\n",
    "acc_hist     = np.mean(y_pred_hist == y_test)\n",
    "acc_hog      = np.mean(y_pred_hog == y_test)\n",
    "acc_lbp      = np.mean(y_pred_lbp == y_test)\n",
    "acc_ensemble = np.mean(y_pred_ensemble == y_test)\n",
    "\n",
    "print(\"\\nTest accuracies on CIFAR-10:\")\n",
    "print(f\" â€¢ Raw (Gaussian NB):       {acc_raw*100:6.2f}%\")\n",
    "print(f\" â€¢ Color Hist (Bernoulli):  {acc_hist*100:6.2f}%\")\n",
    "print(f\" â€¢ HOG (Multinomial):       {acc_hog*100:6.2f}%\")\n",
    "print(f\" â€¢ LBP (Beta NB):           {acc_lbp*100:6.2f}%\")\n",
    "print(f\" â€¢ Ensemble (majority vote):{acc_ensemble*100:6.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
